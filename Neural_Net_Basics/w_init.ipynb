{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autograd import grad \n",
    "import autograd.numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 96)\n",
      "(1, 96)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('data.csv', delimiter=',')\n",
    "x = data[:2]\n",
    "y = data[-1][np.newaxis, :]\n",
    "print(x.shape) #x is 2 features, 96 data points\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#x is shape of (2,96)\n",
    "#it needs to be transposed before standard scale to have batch normalization\n",
    "#x_scaled is shape of (96,2) so it needs to be transposed back\n",
    "x_scaled_intermediate = scaler.fit_transform(x.T) \n",
    "x_scaled = x_scaled_intermediate.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler Raw Code Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjusted x_stds is,  [[1.00269202]\n",
      " [0.25804162]]\n"
     ]
    }
   ],
   "source": [
    "#batch normalization across each data sample.\n",
    "#[:,np.newaxis] is just to add a dimension\n",
    "#x_means has shape (2,1) <- two means aligned in one column. \n",
    "#array([[0.47115037],\n",
    "#       [0.54882423]])\n",
    "#x_stds has shape (2,1)\n",
    "x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
    "x_stds = np.std(x,axis = 1)[:,np.newaxis]  \n",
    "\n",
    "#for experiment, replace std with a small value\n",
    "x_stds = np.array([[0.0026920197],[0.25804162]])\n",
    "\n",
    "\n",
    "# if x_stds < small threshold, add 1 to the where std is small\n",
    "# this is to prevent from dividing a very small std and the output numbers are very large\n",
    "ind = np.argwhere(x_stds < 10**(-2))\n",
    "if len(ind) > 0:\n",
    "    ind = [v[0] for v in ind]\n",
    "    adjust = np.zeros((x_stds.shape))\n",
    "    adjust[ind] = 1.0\n",
    "    x_stds += adjust\n",
    "\n",
    "print(f\"The adjusted x_stds is, \", x_stds)\n",
    "\n",
    "# create standard normalizer function\n",
    "normalizer = lambda data: (data - x_means)/x_stds\n",
    "\n",
    "# create inverse standard normalizer\n",
    "inverse_normalizer = lambda data: data*x_stds + x_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize W matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final W matrix is a list with 2 elements [ , ] \\\n",
    "The second element is the weight for final linear combination in prediction function. \\\n",
    "The first element is another list, which contains the weight for each hidden layer. \\\n",
    "Each weight matrix is an numpy array. \n",
    "\n",
    "\n",
    "Here is the size assumptions: \\\n",
    "N = 2 (two input features) \\\n",
    "M = 1 (one prediction outcome) \\\n",
    "H_1 = 10; H_2 = 10; H_3 = 10 (number of units per hidden layer) \n",
    "\n",
    "\n",
    "Here is the w matrix size (#features, #units in next layer) \\\n",
    "$W_{input->h1}$ = (3, 10) Notice that #features = 3 because it's 2 input features + 1 bias \\\n",
    "$W_{h1->h2}$ = (11, 10) \\\n",
    "$W_{h2->h3}$ = (11, 10) \\\n",
    "$W_{h3->output}$ = (11, 1) \n",
    "\n",
    "W = [[$W_{input->h1}$, $W_{h1->h2}$, $W_{h2->h3}$],$W_{h3->output}$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network_weights(layer_sizes, scale):\n",
    "    weights = []\n",
    "    \n",
    "    for k in range(len(layer_sizes)-1):\n",
    "        # get layer sizes for current weight matrix\n",
    "        U_k = layer_sizes[k]\n",
    "        U_k_plus_1 = layer_sizes[k+1]\n",
    "\n",
    "        # make weight matrix\n",
    "        # np.random.randn follows normal distribution of mean 0 std 1\n",
    "        # times scale here is to change the standard deviation to scale\n",
    "        weight = scale*np.random.randn(U_k+1,U_k_plus_1)\n",
    "        weights.append(weight)\n",
    "\n",
    "    w_init = [weights[:-1],weights[-1]]\n",
    "    \n",
    "    return w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m layer_sizes \u001b[38;5;241m=\u001b[39m [N, U_1,U_2,U_3,M]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# generate initial weights for our network\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m w_init \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_network_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36minitialize_network_weights\u001b[0;34m(layer_sizes, scale)\u001b[0m\n\u001b[1;32m      7\u001b[0m     U_k_plus_1 \u001b[38;5;241m=\u001b[39m layer_sizes[k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# make weight matrix\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# np.random.randn follows normal distribution of mean 0 std 1\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# times scale here is to change the standard deviation to scale\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     weight \u001b[38;5;241m=\u001b[39m scale\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(U_k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,U_k_plus_1)\n\u001b[1;32m     13\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(weight)\n\u001b[1;32m     15\u001b[0m w_init \u001b[38;5;241m=\u001b[39m [weights[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],weights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# An example 4 hidden layer network, with 10 units in each layer\n",
    "N = 2  # dimension of input\n",
    "M = 1  # dimension of output\n",
    "U_1 = 10; U_2 = 10; U_3 = 10;  # number of units per hidden layer\n",
    "# the list defines our network architecture\n",
    "layer_sizes = [N, U_1,U_2,U_3,M]\n",
    "# generate initial weights for our network\n",
    "w_init = initialize_network_weights(layer_sizes, scale = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W matrix size,  2\n",
      "W first element size,  3\n",
      "W first elements shape,  (3, 10) (11, 10) (11, 10)\n",
      "W second element shape,  (11, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"W matrix size, \", len(w_init))\n",
    "print(f\"W first element size, \", len(w_init[0]))\n",
    "print(f\"W first elements shape, \", w_init[0][0].shape, w_init[0][1].shape, w_init[0][2].shape)\n",
    "print(f\"W second element shape, \", w_init[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### two-class classification costs #######\n",
    "# the convex softmax cost function\n",
    "def softmax(w,x,y,iter):\n",
    "    # compute cost over batch\n",
    "    cost = np.sum(np.log(1 + np.exp(-y*model(x,w))))\n",
    "    return cost/float(np.size(y))\n",
    "\n",
    "def model(x,w):   \n",
    "    # feature transformation - switch for dealing\n",
    "    # with feature transforms that either do or do\n",
    "    # not have internal parameters\n",
    "    f = 0\n",
    "    f = feature_transforms(x,w[0]) \n",
    "\n",
    "    # compute linear combination and return\n",
    "    # switch for dealing with feature transforms that either \n",
    "    # do or do not have internal parameters\n",
    "    a = 0\n",
    "    a = w[1][0] + np.dot(f.T,w[1][1:])\n",
    "    return a.T\n",
    "\n",
    "def feature_transforms(a, w):    \n",
    "    activation = lambda data: np.tanh(data)\n",
    "    # loop through each layer matrix\n",
    "    for W in w:\n",
    "        # compute inner product with current layer weights\n",
    "        a = W[0] + np.dot(a.T, W[1:])\n",
    "\n",
    "        # output of layer activation\n",
    "        a = activation(a).T\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function flatten_func.<locals>.<lambda> at 0x15408e670>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/jcvr35lj59lcykv3srmmv9p00000gn/T/ipykernel_77139/1468230978.py:92: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cost/float(np.size(y_p))\n"
     ]
    }
   ],
   "source": [
    "max_its = 1000\n",
    "alpha_choice = 0.1\n",
    "\n",
    "train_num = batch_size = np.size(y)\n",
    "weight_histories = []\n",
    "train_cost_histories = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "weight_history,train_cost_history,val_cost_history = super_optimizers_gradient_descent(mylib5.cost,w_init,x_scaled,y,x_val,y_val,alpha_choice,max_its,batch_size,verbose=False)\n",
    "weight_histories.append(weight_history)\n",
    "train_cost_histories.append(train_cost_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/jcvr35lj59lcykv3srmmv9p00000gn/T/ipykernel_77139/1468230978.py:92: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return cost/float(np.size(y_p))\n",
      "/var/folders/c3/jcvr35lj59lcykv3srmmv9p00000gn/T/ipykernel_77139/4149735772.py:160: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  val_accuracy_history = [1 - self.counter(v,self.x_val,self.y_val)/float(self.y_val.size) for v in weight_history]\n"
     ]
    }
   ],
   "source": [
    "mylib5 = Setup(x,y)\n",
    "\n",
    "mylib5.preprocessing_steps(normalizer = 'standard')\n",
    "\n",
    "# split into training and validation sets\n",
    "mylib5.make_train_val_split(train_portion = 1)\n",
    "\n",
    "# choose cost\n",
    "mylib5.choose_cost(name = 'softmax')\n",
    "\n",
    "# choose dimensions of fully connected multilayer perceptron layers\n",
    "layer_sizes = [10,10,10]\n",
    "mylib5.choose_features(feature_name = 'multilayer_perceptron',layer_sizes = layer_sizes,activation = 'tanh',scale = 0.5)\n",
    "\n",
    "# fit an optimization\n",
    "mylib5.fit(max_its = 1000,alpha_choice = 10**(-1),verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "import autograd.numpy as np\n",
    "from inspect import signature\n",
    "from autograd import value_and_grad \n",
    "from autograd import hessian\n",
    "from autograd.misc.flatten import flatten_func\n",
    "from IPython.display import clear_output\n",
    "from timeit import default_timer as timer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Super Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Setup:\n",
    "    def __init__(self,x,y,**kwargs):\n",
    "        # link in data\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # make containers for all histories\n",
    "        self.weight_histories = []\n",
    "        self.train_cost_histories = []\n",
    "        self.train_accuracy_histories = []\n",
    "        self.val_cost_histories = []\n",
    "        self.val_accuracy_histories = []\n",
    "        self.train_costs = []\n",
    "        self.train_counts = []\n",
    "        self.val_costs = []\n",
    "        self.val_counts = []\n",
    "        \n",
    "    #### define preprocessing steps ####\n",
    "    def preprocessing_steps(self,**kwargs):        \n",
    "        ### produce / use data normalizer ###\n",
    "        normalizer_name = 'standard'\n",
    "        if 'normalizer_name' in kwargs:\n",
    "            normalizer_name = kwargs['normalizer_name']\n",
    "        self.normalizer_name = normalizer_name\n",
    "\n",
    "        # produce normalizer / inverse normalizer\n",
    "        s = normalizers_Setup(self.x,normalizer_name)\n",
    "        self.normalizer = s.normalizer\n",
    "        self.inverse_normalizer = s.inverse_normalizer\n",
    "        \n",
    "        # normalize input \n",
    "        self.x = self.normalizer(self.x)\n",
    "       \n",
    "    #### split data into training and validation sets ####\n",
    "    def make_train_val_split(self,train_portion):\n",
    "        # translate desired training portion into exact indecies\n",
    "        self.train_portion = train_portion\n",
    "        r = np.random.permutation(self.x.shape[1])\n",
    "        train_num = int(np.round(train_portion*len(r)))\n",
    "        self.train_inds = r[:train_num]\n",
    "        self.val_inds = r[train_num:]\n",
    "        \n",
    "        # define training and testing sets\n",
    "        self.x_train = self.x[:,self.train_inds]\n",
    "        self.x_val = self.x[:,self.val_inds]\n",
    "\n",
    "        self.y_train = self.y[:,self.train_inds]\n",
    "        self.y_val = self.y[:,self.val_inds]\n",
    "     \n",
    "    #### define cost function ####\n",
    "    def choose_cost(self,name,**kwargs):\n",
    "        # create training and testing cost functions\n",
    "        self.cost_object = super_cost_functions_Setup(name,**kwargs)\n",
    "\n",
    "        # if the cost function is a two-class classifier, build a counter too\n",
    "        if name == 'softmax' or name == 'perceptron':\n",
    "            self.count_object = super_cost_functions_Setup('twoclass_counter',**kwargs)\n",
    "                        \n",
    "        if name == 'multiclass_softmax' or name == 'multiclass_perceptron':\n",
    "            self.count_object = super_cost_functions_Setup('multiclass_counter',**kwargs)\n",
    "  \n",
    "        self.cost_name = name\n",
    "    \n",
    "    #### define feature transformation ####\n",
    "    def choose_features(self,**kwargs): \n",
    "        ### select from pre-made feature transforms ###\n",
    "        layer_sizes = [1]\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        \n",
    "        # add input and output layer sizes\n",
    "        input_size = self.x.shape[0]\n",
    "        layer_sizes.insert(0, input_size)\n",
    "      \n",
    "        # add output size\n",
    "        if self.cost_name == 'least_squares' or self.cost_name == 'least_absolute_deviations':\n",
    "            layer_sizes.append(self.y.shape[0])\n",
    "        else:\n",
    "            num_labels = len(np.unique(self.y))\n",
    "            if num_labels == 2:\n",
    "                layer_sizes.append(1)\n",
    "            else:\n",
    "                layer_sizes.append(num_labels)\n",
    "        \n",
    "        # multilayer perceptron #\n",
    "        feature_name = 'multilayer_perceptron'\n",
    "        if 'name' in kwargs:\n",
    "            feature_name = kwargs['feature_name']\n",
    "           \n",
    "        if feature_name == 'multilayer_perceptron':\n",
    "            transformer = multilayer_perceptron_Setup(**kwargs)\n",
    "            self.feature_transforms = transformer.feature_transforms\n",
    "            self.multilayer_initializer = transformer.initializer\n",
    "            self.layer_sizes = transformer.layer_sizes\n",
    "            \n",
    "        self.feature_name = feature_name\n",
    "        \n",
    "        ### with feature transformation constructed, pass on to cost function ###\n",
    "        self.cost_object.define_feature_transform(self.feature_transforms)\n",
    "        self.cost = self.cost_object.cost\n",
    "        self.model = self.cost_object.model\n",
    "        \n",
    "        # if classification performed, inject feature transforms into counter as well\n",
    "        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n",
    "            self.count_object.define_feature_transform(self.feature_transforms)\n",
    "            self.counter = self.count_object.cost\n",
    "            \n",
    "    #### run optimization ####\n",
    "    def fit(self,**kwargs):\n",
    "        # basic parameters for gradient descent run (default algorithm)\n",
    "        max_its = 500; alpha_choice = 10**(-1);\n",
    "        \n",
    "        # set parameters by hand\n",
    "        if 'max_its' in kwargs:\n",
    "            self.max_its = kwargs['max_its']\n",
    "        if 'alpha_choice' in kwargs:\n",
    "            self.alpha_choice = kwargs['alpha_choice']\n",
    "        \n",
    "        # set initialization\n",
    "        self.w_init = self.multilayer_initializer()\n",
    "        \n",
    "        # batch size for gradient descent?\n",
    "        self.train_num = np.size(self.y_train)\n",
    "        self.val_num = np.size(self.y_val)\n",
    "        self.batch_size = np.size(self.y_train)\n",
    "        if 'batch_size' in kwargs:\n",
    "            self.batch_size = min(kwargs['batch_size'],self.batch_size)\n",
    "        \n",
    "        # verbose or not\n",
    "        verbose = True\n",
    "        if 'verbose' in kwargs:\n",
    "            verbose = kwargs['verbose']\n",
    "\n",
    "        # optimize\n",
    "        weight_history = []\n",
    "        cost_history = []\n",
    "        \n",
    "        # run gradient descent\n",
    "        weight_history,train_cost_history,val_cost_history = super_optimizers_gradient_descent(self.cost,self.w_init,self.x_train,self.y_train,self.x_val,self.y_val,self.alpha_choice,self.max_its,self.batch_size,verbose=verbose)\n",
    "                                                                                         \n",
    "        # store all new histories\n",
    "        self.weight_histories.append(weight_history)\n",
    "        self.train_cost_histories.append(train_cost_history)\n",
    "        self.val_cost_histories.append(val_cost_history)\n",
    "\n",
    "        # if classification produce count history\n",
    "        if self.cost_name == 'softmax' or self.cost_name == 'perceptron' or self.cost_name == 'multiclass_softmax' or self.cost_name == 'multiclass_perceptron':\n",
    "            train_accuracy_history = [1 - self.counter(v,self.x_train,self.y_train)/float(self.y_train.size) for v in weight_history]\n",
    "            val_accuracy_history = [1 - self.counter(v,self.x_val,self.y_val)/float(self.y_val.size) for v in weight_history]\n",
    "\n",
    "            # store count history\n",
    "            self.train_accuracy_histories.append(train_accuracy_history)\n",
    "            self.val_accuracy_histories.append(val_accuracy_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### super_optimizers_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_optimizers_gradient_descent(g,w,x_train,y_train,x_val,y_val,alpha,max_its,batch_size,**kwargs): \n",
    "    verbose = True\n",
    "    if 'verbose' in kwargs:\n",
    "        verbose = kwargs['verbose']\n",
    "    \n",
    "    # flatten the input function, create gradient based on flat function\n",
    "    g_flat, unflatten, w = flatten_func(g, w)\n",
    "    grad = value_and_grad(g_flat)\n",
    "\n",
    "    # record history\n",
    "    num_train = y_train.size\n",
    "    num_val = y_val.size\n",
    "    w_hist = [unflatten(w)]\n",
    "    train_hist = [g_flat(w,x_train,y_train,np.arange(num_train))]\n",
    "    val_hist = [g_flat(w,x_val,y_val,np.arange(num_val))]\n",
    "\n",
    "    # how many mini-batches equal the entire dataset?\n",
    "    num_batches = int(np.ceil(np.divide(num_train, batch_size)))\n",
    "\n",
    "    # over the line\n",
    "    for k in range(max_its):                   \n",
    "        # loop over each minibatch\n",
    "        start = timer()\n",
    "        train_cost = 0\n",
    "        for b in range(num_batches):\n",
    "            # collect indices of current mini-batch\n",
    "            batch_inds = np.arange(b*batch_size, min((b+1)*batch_size, num_train))\n",
    "            \n",
    "            # plug in value into func and derivative\n",
    "            cost_eval,grad_eval = grad(w,x_train,y_train,batch_inds)\n",
    "            grad_eval.shape = np.shape(w)\n",
    "    \n",
    "            # take descent step with momentum\n",
    "            w = w - alpha*grad_eval\n",
    "\n",
    "        end = timer()\n",
    "        \n",
    "        # update training and validation cost\n",
    "        train_cost = g_flat(w,x_train,y_train,np.arange(num_train))\n",
    "        val_cost = g_flat(w,x_val,y_val,np.arange(num_val))\n",
    "\n",
    "        # record weight update, train and val costs\n",
    "        w_hist.append(unflatten(w))\n",
    "        train_hist.append(train_cost)\n",
    "        val_hist.append(val_cost)\n",
    "\n",
    "        if verbose == True:\n",
    "            print ('step ' + str(k+1) + ' done in ' + str(np.round(end - start,1)) + ' secs, train cost = ' + str(np.round(train_hist[-1][0],4)) + ', val cost = ' + str(np.round(val_hist[-1][0],4)))\n",
    "\n",
    "    if verbose == True:\n",
    "        print ('finished all ' + str(max_its) + ' steps')\n",
    "        #time.sleep(1.5)\n",
    "        #clear_output()\n",
    "    return w_hist,train_hist,val_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### super_cost_functions_Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class super_cost_functions_Setup:\n",
    "    def __init__(self,name,**kwargs):        \n",
    "        ### make cost function choice ###\n",
    "        # for regression\n",
    "        if name == 'least_squares':\n",
    "            self.cost = self.least_squares\n",
    "        if name == 'least_absolute_deviations':\n",
    "            self.cost = self.least_absolute_deviations\n",
    "            \n",
    "        # for two-class classification\n",
    "        if name == 'softmax':\n",
    "            self.cost = self.softmax\n",
    "        if name == 'perceptron':\n",
    "            self.cost = self.perceptron\n",
    "        if name == 'twoclass_counter':\n",
    "            self.cost = self.counting_cost\n",
    "            \n",
    "        # for multiclass classification\n",
    "        if name == 'multiclass_perceptron':\n",
    "            self.cost = self.multiclass_perceptron\n",
    "        if name == 'multiclass_softmax':\n",
    "            self.cost = self.multiclass_softmax\n",
    "        if name == 'multiclass_counter':\n",
    "            self.cost = self.multiclass_counting_cost\n",
    "            \n",
    "        # for autoencoder\n",
    "        if name == 'autoencoder':\n",
    "            self.feature_transforms = feature_transforms\n",
    "            self.feature_transforms_2 = kwargs['feature_transforms_2']\n",
    "            self.cost = self.autoencoder\n",
    "            \n",
    "    ### insert feature transformations to use ###\n",
    "    def define_feature_transform(self,feature_transforms):\n",
    "        # make copy of feature transformation\n",
    "        self.feature_transforms = feature_transforms\n",
    "        \n",
    "        # count parameter layers of input to feature transform\n",
    "        self.sig = signature(self.feature_transforms)\n",
    "            \n",
    "    ##### models functions #####\n",
    "    # compute linear combination of features\n",
    "    def model(self,x,w):   \n",
    "        # feature transformation - switch for dealing\n",
    "        # with feature transforms that either do or do\n",
    "        # not have internal parameters\n",
    "        f = 0\n",
    "        if len(self.sig.parameters) == 2:\n",
    "            f = self.feature_transforms(x,w[0])\n",
    "        else: \n",
    "            f = self.feature_transforms(x)    \n",
    "\n",
    "        # compute linear combination and return\n",
    "        # switch for dealing with feature transforms that either \n",
    "        # do or do not have internal parameters\n",
    "        a = 0\n",
    "        if len(self.sig.parameters) == 2:\n",
    "            a = w[1][0] + np.dot(f.T,w[1][1:])\n",
    "        else:\n",
    "            a = w[0] + np.dot(f.T,w[1:])\n",
    "        return a.T\n",
    "\n",
    "    ###### regression costs #######\n",
    "    # an implementation of the least squares cost function for linear regression\n",
    "    def least_squares(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "                \n",
    "        # compute cost over batch\n",
    "        cost = np.sum((self.model(x_p,w) - y_p)**2)\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # a compact least absolute deviations cost function\n",
    "    def least_absolute_deviations(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "\n",
    "        # compute cost over batch\n",
    "        cost = np.sum(np.abs(self.model(x_p,w) - y_p))\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    ###### two-class classification costs #######\n",
    "    # the convex softmax cost function\n",
    "    def softmax(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "        \n",
    "        # compute cost over batch\n",
    "        cost = np.sum(np.log(1 + np.exp(-y_p*self.model(x_p,w))))\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # the convex relu cost function\n",
    "    def relu(self,w,x,y,iter):\n",
    "        # get batch of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "        \n",
    "        # compute cost over batch\n",
    "        cost = np.sum(np.maximum(0,-y_p*self.model(x_p,w)))\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # the counting cost function\n",
    "    def counting_cost(self,w,x,y):\n",
    "        cost = np.sum((np.sign(self.model(x,w)) - y)**2)\n",
    "        return 0.25*cost \n",
    "\n",
    "    ###### multiclass classification costs #######\n",
    "    # multiclass perceptron\n",
    "    def multiclass_perceptron(self,w,x,y,iter):\n",
    "        # get subset of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "\n",
    "        # pre-compute predictions on all points\n",
    "        all_evals = self.model(x_p,w)\n",
    "\n",
    "        # compute maximum across data points\n",
    "        a =  np.max(all_evals,axis = 0)        \n",
    "\n",
    "        # compute cost in compact form using numpy broadcasting\n",
    "        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
    "        cost = np.sum(a - b)\n",
    "\n",
    "        # return average\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # multiclass softmax\n",
    "    def multiclass_softmax(self,w,x,y,iter):     \n",
    "        # get subset of points\n",
    "        x_p = x[:,iter]\n",
    "        y_p = y[:,iter]\n",
    "        \n",
    "        # pre-compute predictions on all points\n",
    "        all_evals = self.model(x_p,w)\n",
    "\n",
    "        # compute softmax across data points\n",
    "        a = np.log(np.sum(np.exp(all_evals),axis = 0)) \n",
    "\n",
    "        # compute cost in compact form using numpy broadcasting\n",
    "        b = all_evals[y_p.astype(int).flatten(),np.arange(np.size(y_p))]\n",
    "        cost = np.sum(a - b)\n",
    "\n",
    "        # return average\n",
    "        return cost/float(np.size(y_p))\n",
    "\n",
    "    # multiclass misclassification cost function - aka the fusion rule\n",
    "    def multiclass_counting_cost(self,w,x,y):                \n",
    "        # pre-compute predictions on all points\n",
    "        all_evals = self.model(x,w)\n",
    "\n",
    "        # compute predictions of each input point\n",
    "        y_predict = (np.argmax(all_evals,axis = 0))[np.newaxis,:]\n",
    "\n",
    "        # compare predicted label to actual label\n",
    "        count = np.sum(np.abs(np.sign(y - y_predict)))\n",
    "\n",
    "        # return number of misclassifications\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalizers_Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalizers_Setup:\n",
    "    def __init__(self,x,name):\n",
    "        normalizer = 0\n",
    "        inverse_normalizer = 0\n",
    "        \n",
    "        # use standard normalizer\n",
    "        if name == 'standard':\n",
    "            self.normalizer, self.inverse_normalizer = self.standard_normalizer(x)\n",
    "            \n",
    "        # use PCA sphereing\n",
    "        elif name == 'PCA_sphere':\n",
    "            # create normalizer\n",
    "            self.normalizer, self.inverse_normalizer = self.PCA_sphereing(x)\n",
    "        \n",
    "        # use ZCA sphereing\n",
    "        elif name == 'ZCA_sphere':\n",
    "            self.normalizer, self.inverse_normalizer = self.ZCA_sphereing(x)\n",
    "            \n",
    "        else:\n",
    "            self.normalizer = lambda data: data\n",
    "            self.inverse_normalizer = lambda data: data\n",
    "            \n",
    "    # standard normalization function \n",
    "    def standard_normalizer(self,x):\n",
    "        # compute the mean and standard deviation of the input\n",
    "        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
    "        x_stds = np.std(x,axis = 1)[:,np.newaxis]   \n",
    "        \n",
    "        # check to make sure thta x_stds > small threshold, for those not\n",
    "        # divide by 1 instead of original standard deviation\n",
    "        ind = np.argwhere(x_stds < 10**(-2))\n",
    "        if len(ind) > 0:\n",
    "            ind = [v[0] for v in ind]\n",
    "            adjust = np.zeros((x_stds.shape))\n",
    "            adjust[ind] = 1.0\n",
    "            x_stds += adjust\n",
    "\n",
    "        # create standard normalizer function\n",
    "        normalizer = lambda data: (data - x_means)/x_stds\n",
    "\n",
    "        # create inverse standard normalizer\n",
    "        inverse_normalizer = lambda data: data*x_stds + x_means\n",
    "\n",
    "        # return normalizer \n",
    "        return normalizer,inverse_normalizer\n",
    "\n",
    "    # compute eigendecomposition of data covariance matrix for PCA transformation\n",
    "    def PCA(self,x,**kwargs):\n",
    "        # regularization parameter for numerical stability\n",
    "        lam = 10**(-7)\n",
    "        if 'lam' in kwargs:\n",
    "            lam = kwargs['lam']\n",
    "\n",
    "        # create the correlation matrix\n",
    "        P = float(x.shape[1])\n",
    "        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n",
    "\n",
    "        # use numpy function to compute eigenvalues / vectors of correlation matrix\n",
    "        d,V = np.linalg.eigh(Cov)\n",
    "        return d,V\n",
    "\n",
    "    # PCA-sphereing - use PCA to normalize input features\n",
    "    def PCA_sphereing(self,x,**kwargs):\n",
    "        # Step 1: mean-center the data\n",
    "        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
    "        x_centered = x - x_means\n",
    "\n",
    "        # Step 2: compute pca transform on mean-centered data\n",
    "        d,V = self.PCA(x_centered,**kwargs)\n",
    "\n",
    "        # Step 3: divide off standard deviation of each (transformed) input, \n",
    "        # which are equal to the returned eigenvalues in 'd'.  \n",
    "        stds = (d[:,np.newaxis])**(0.5)\n",
    "        \n",
    "        # check to make sure thta x_stds > small threshold, for those not\n",
    "        # divide by 1 instead of original standard deviation\n",
    "        ind = np.argwhere(stds < 10**(-2))\n",
    "        if len(ind) > 0:\n",
    "            ind = [v[0] for v in ind]\n",
    "            adjust = np.zeros((stds.shape))\n",
    "            adjust[ind] = 1.0\n",
    "            stds += adjust\n",
    "        \n",
    "        normalizer = lambda data: np.dot(V.T,data - x_means)/stds\n",
    "\n",
    "        # create inverse normalizer\n",
    "        inverse_normalizer = lambda data: np.dot(V,data*stds) + x_means\n",
    "\n",
    "        # return normalizer \n",
    "        return normalizer,inverse_normalizer\n",
    "    \n",
    "    \n",
    "    # ZCA-sphereing - use ZCA to normalize input features\n",
    "    def ZCA_sphereing(self,x,**kwargs):\n",
    "        # Step 1: mean-center the data\n",
    "        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
    "        x_centered = x - x_means\n",
    "\n",
    "        # Step 2: compute pca transform on mean-centered data\n",
    "        d,V = self.PCA(x_centered,**kwargs)\n",
    "\n",
    "        # Step 3: divide off standard deviation of each (transformed) input, \n",
    "        # which are equal to the returned eigenvalues in 'd'.  \n",
    "        stds = (d[:,np.newaxis])**(0.5)\n",
    "        \n",
    "        # check to make sure thta x_stds > small threshold, for those not\n",
    "        # divide by 1 instead of original standard deviation\n",
    "        ind = np.argwhere(stds < 10**(-2))\n",
    "        if len(ind) > 0:\n",
    "            ind = [v[0] for v in ind]\n",
    "            adjust = np.zeros((stds.shape))\n",
    "            adjust[ind] = 1.0\n",
    "            stds += adjust\n",
    "             \n",
    "        normalizer = lambda data: np.dot(V, np.dot(V.T,data - x_means)/stds)\n",
    "\n",
    "        # create inverse normalizer\n",
    "        inverse_normalizer = lambda data: np.dot(V,np.dot(V.T,data)*stds) + x_means\n",
    "\n",
    "        # return normalizer \n",
    "        return normalizer,inverse_normalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multilayer_perceptron_Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multilayer_perceptron_Setup:\n",
    "    def __init__(self,**kwargs):  \n",
    "        # set default values for layer sizes, activation, and scale\n",
    "        activation = 'relu'\n",
    "\n",
    "        # decide on these parameters via user input\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # switches\n",
    "        if activation == 'linear':\n",
    "            self.activation = lambda data: data\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = lambda data: np.tanh(data)\n",
    "        elif activation == 'relu':\n",
    "            self.activation = lambda data: np.maximum(0,data)\n",
    "        elif activation == 'sinc':\n",
    "            self.activation = lambda data: np.sinc(data)\n",
    "        elif activation == 'sin':\n",
    "            self.activation = lambda data: np.sin(data)\n",
    "        elif activation == 'maxout':\n",
    "            self.activation = lambda data1,data2: np.maximum(data1,data2)\n",
    "                        \n",
    "        # get layer sizes\n",
    "        layer_sizes = kwargs['layer_sizes']\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.scale = 0.1\n",
    "        if 'scale' in kwargs:\n",
    "            self.scale = kwargs['scale']\n",
    "            \n",
    "        # assign initializer / feature transforms function\n",
    "        if activation == 'linear' or activation == 'tanh' or activation == 'relu' or activation == 'sinc' or activation == 'sin':\n",
    "            self.initializer = self.standard_initializer\n",
    "            self.feature_transforms = self.standard_feature_transforms\n",
    "        elif activation == 'maxout':\n",
    "            self.initializer = self.maxout_initializer\n",
    "            self.feature_transforms = self.maxout_feature_transforms\n",
    "\n",
    "    ####### initializers ######\n",
    "    # create initial weights for arbitrary feedforward network\n",
    "    def standard_initializer(self):\n",
    "        # container for entire weight tensor\n",
    "        weights = []\n",
    "\n",
    "        # loop over desired layer sizes and create appropriately sized initial \n",
    "        # weight matrix for each layer\n",
    "        for k in range(len(self.layer_sizes)-1):\n",
    "            # get layer sizes for current weight matrix\n",
    "            U_k = self.layer_sizes[k]\n",
    "            U_k_plus_1 = self.layer_sizes[k+1]\n",
    "\n",
    "            # make weight matrix\n",
    "            weight = self.scale*np.random.randn(U_k+1,U_k_plus_1)\n",
    "            weights.append(weight)\n",
    "\n",
    "        # re-express weights so that w_init[0] = omega_inner contains all \n",
    "        # internal weight matrices, and w_init = w contains weights of \n",
    "        # final linear combination in predict function\n",
    "        w_init = [weights[:-1],weights[-1]]\n",
    "\n",
    "        return w_init\n",
    "    \n",
    "    # create initial weights for arbitrary feedforward network\n",
    "    def maxout_initializer(self):\n",
    "        # container for entire weight tensor\n",
    "        weights = []\n",
    "\n",
    "        # loop over desired layer sizes and create appropriately sized initial \n",
    "        # weight matrix for each layer\n",
    "        for k in range(len(self.layer_sizes)-1):\n",
    "            # get layer sizes for current weight matrix\n",
    "            U_k = self.layer_sizes[k]\n",
    "            U_k_plus_1 = self.layer_sizes[k+1]\n",
    "\n",
    "            # make weight matrix\n",
    "            weight1 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n",
    "\n",
    "            # add second matrix for inner weights\n",
    "            if k < len(self.layer_sizes)-2:\n",
    "                weight2 = self.scale*np.random.randn(U_k + 1,U_k_plus_1)\n",
    "                weights.append([weight1,weight2])\n",
    "            else:\n",
    "                weights.append(weight1)\n",
    "\n",
    "        # re-express weights so that w_init[0] = omega_inner contains all \n",
    "        # internal weight matrices, and w_init = w contains weights of \n",
    "        # final linear combination in predict function\n",
    "        w_init = [weights[:-1],weights[-1]]\n",
    "\n",
    "        return w_init\n",
    "\n",
    "    ####### feature transforms ######\n",
    "    # fully evaluate our network features using the tensor of weights in w\n",
    "    def standard_feature_transforms(self,a, w):    \n",
    "        # loop through each layer matrix\n",
    "        for W in w:\n",
    "            # compute inner product with current layer weights\n",
    "            a = W[0] + np.dot(a.T, W[1:])\n",
    "\n",
    "            # output of layer activation\n",
    "            a = self.activation(a).T\n",
    "        return a\n",
    "    \n",
    "    # fully evaluate our network features using the tensor of weights in w\n",
    "    def maxout_feature_transforms(self,a, w):    \n",
    "        # loop through each layer matrix\n",
    "        for W1,W2 in w:\n",
    "            # compute inner product with current layer weights\n",
    "            a1 = W1[0][:,np.newaxis] + np.dot(a.T, W1[1:]).T\n",
    "            a2 = W2[0][:,np.newaxis] + np.dot(a.T, W2[1:]).T\n",
    "\n",
    "            # output of layer activation\n",
    "            a = self.activation(a1,a2)\n",
    "        return a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
